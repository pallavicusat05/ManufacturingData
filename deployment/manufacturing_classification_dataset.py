# -*- coding: utf-8 -*-
"""Manufacturing Classification Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F0hsRkdE9eF-AdMnWbVnQxZD73jzgycC

**This is the data from a semiconductor manufacturing process. We will analyze whether all the features are required to build the model or not and build a classifier to predict the Pass/Fail yield of a particular process entity.**

# Machine Learning Approach
---

# Exploratory Data Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary libraries
import os
import joblib
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

sns.set_style('darkgrid')
sns.set()
import plotly.express as px
import warnings

warnings.filterwarnings('ignore')
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay, \
    RocCurveDisplay, PrecisionRecallDisplay, precision_recall_curve, roc_curve

# Reading the data from csv file and checking first five rows
df = pd.read_csv('data/manufacturing_dataset.csv')
df.head()

df.info()

print(df.shape)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns\n')

pd.set_option("display.max_columns", 600)
pd.set_option("display.width", 1100)

df.head()

# Keeping a copy of dataset for backup
dataset = df.copy()
dataset.shape

# Renaming the features columns to features_<number> instead of <number>
df.columns = 'features_' + df.columns

df.rename(columns={'features_Time': 'Time'}, inplace=True)
df.rename(columns={'features_Pass/Fail': 'Pass/Fail'}, inplace=True)

df.head(2)

# Checking the datatype of the columns (features)
pd.DataFrame(df.dtypes).transpose()

df['Pass/Fail'].unique()

"""All the features are 'float', except below two columns:

Time -> 'object' format

Pass/Fail -> 'int64' format (it has only two values 1 and -1)
"""

# Displaying the statistics of all the columns (count, mean, standard deviation, minimum, maximum, Quartile1(25%), Quartile2/Median(50%), Quartile3(75%))
df.describe()

"""**'Pass/Fail' is the target column. We will check it's distribution, to check for Class Imbalance:**"""

df['Pass/Fail'].value_counts()

# Examining Class Imbalance
df['Pass/Fail'].value_counts(normalize=True)

df['Pass/Fail'].value_counts().plot(kind='bar')
plt.show()

fig = px.pie(
    df['Pass/Fail'].value_counts(),
    values='Pass/Fail',
    names=["PASS", "FAIL"],
    title="Class Distribution",
    width=500
)

fig.show()

# Heatmap to check multi-collinearity of data columns
plt.figure(figsize=(20, 12))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()


# As there lot of features, we are not able to get much from heatmap. We will get collinearity in tabular format and accordingly pre-process the data later

# Checking box-plot for distribution of features columns
def boxplots(col):
    sns.boxplot(df[col])
    plt.show()


for i in list(df.select_dtypes(exclude=['object']).columns)[1:-1]:
    boxplots(i)


# Checking distribution plot of all features columns
def distplots(col):
    sns.displot(df[col])
    plt.show()


for i in list(df.columns)[1:-1]:
    distplots(i)

"""We will do some more analysis on the 'Pass/Fail' data based on time"""

df['year'] = pd.DatetimeIndex(df['Time']).year
df['month'] = pd.DatetimeIndex(df['Time']).month
df['date'] = pd.DatetimeIndex(df['Time']).day
df['week_day'] = pd.DatetimeIndex(df['Time']).weekday
df['start_time'] = pd.DatetimeIndex(df['Time']).time
df['hour'] = pd.DatetimeIndex(df['Time']).hour
df['mins'] = pd.DatetimeIndex(df['Time']).minute
df.head()

df.year.unique()

df.month.unique()

df.date.unique()

df.week_day.unique()

sns.displot(df[df['Pass/Fail'] == -1]['month'], color='r')
sns.displot(df[df['Pass/Fail'] == 1]['month'], color='b')
plt.show()

sns.displot(df[df['Pass/Fail'] == -1]['date'], color='r')
sns.displot(df[df['Pass/Fail'] == 1]['date'], color='b')
plt.show()

sns.displot(df[df['Pass/Fail'] == -1]['hour'], color='r')
sns.displot(df[df['Pass/Fail'] == 1]['hour'], color='b')
plt.show()

"""Dropping columns related to date and time, since those are not required for model building"""

df = df.drop(['year', 'month', 'date', 'week_day', 'start_time', 'hour', 'mins'], axis=1)

"""# Data Preprocessing

Dropping 'Time' column, as it does not provide much value
"""

df = df.drop(['Time'], axis=1)

"""Since there is lot of extra columns (features) which has missing values and many of them might not make sense, so need to drop them."""

missing_per = df.isna().sum() * 100 / df.shape[0]
fig = px.line(x=missing_per.index, y=missing_per, title="Percentage of missing values in all the features")
fig.update_xaxes(title_text='Features')
fig.update_yaxes(title_text='Percentage of Missing values', range=[0, 100])
fig.show()

"""We can see that there are very high percentage of missing data for some features."""

pd.DataFrame(df.isnull().sum().sort_values(ascending=False)).transpose()

# Precentage of missing value for each feature
pd.DataFrame((df.isnull().sum() * 100 / df.shape[0]).sort_values(ascending=False)).transpose()


# There are many features with more than 40% missing values. Those will not provide much insight in prediction, so needs to be dropped
def remove_null_columns(data, thres):
    columns = data.columns
    cols_remove = []
    for i in columns:
        if (data[i].isna().sum() / data.shape[0] >= thres):
            cols_remove.append(i)
    print(f'Number of features removed with more than {thres}% of null values : \t {len(cols_remove)}')
    data = data.drop(labels=cols_remove, axis=1)
    return (data)


df = remove_null_columns(df, 0.4)

df.shape

# There are many columns with single unique value. These columns need to be dropped as well, as they provide value in classification
uni_list = []
for column in df.columns:
    if (df[column].nunique() == 1):
        uni_list.append(column)
print(f'Number of features with single unique value removed : \t {len(uni_list)}')
df.drop(columns=uni_list, axis=1, inplace=True)

df.shape

# Since we could net get much information from Heatmap, so checking the correlation as below and importing it to csv file
corr = df.corr()
print(corr)

# Importing correlation matrix to the csv file
corr.to_csv('output_correlation.csv')


# We can select highly correlated features with the following function
# It will remove the first feature that is correlated with any other feature
def correlation(dataframe, threshold):
    # Set of all the names of correlated columns
    col_corr = set()
    corr_matrix = dataframe.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            # Getting the absolute correlation coefficient value
            if abs(corr_matrix.iloc[i, j]) > threshold:
                # Getting the name of the first feature that is correlated with any other feature
                colname = corr_matrix.columns[i]
                col_corr.add(colname)
    return col_corr


# Removing features having more than 70% correlation from the original DataFrame
# Both positive and negative correlations are considered here
corr_features = correlation(df, 0.7)
df = df.drop(corr_features, axis=1)
print(f'After removing {len(corr_features)} correlated features, there are {df.shape[1]} features left.')

df.shape


# There are several highly multicollinear features. We can remove these features as well.
# It will get the list of all the highly multicollinear features having VIF greater than vif_threshold
def high_vif_features(dataframe, vif_threshold):
    dataframe = dataframe.dropna()
    # Create a DataFrame to store VIF values and their corresponding features
    vif_data = pd.DataFrame()
    vif_data["feature"] = dataframe.columns
    vif_data["VIF"] = [variance_inflation_factor(dataframe.values, i) for i in range(dataframe.shape[1])]
    # Find features with VIF greater than the threshold
    high_vif_columns = vif_data[vif_data["VIF"] > vif_threshold]["feature"].tolist()
    return high_vif_columns


# Removing high VIF features (having VIF greater than 10) from the original DataFrame
high_vif_cols = high_vif_features(df, 10)
df = df.drop(high_vif_cols, axis=1)
print(f'After removing {len(high_vif_cols)} highly multicollinear features, there are {df.shape[1]} features left.')

df.shape


# We would like our features to have high correlation with the target.
# If a feature has low correlation with target, it means that it is not a helpful feature for predicting the target, and hence, should be removed.
# It will remove the features having low correlation (less than threshold) with the target
def corr_with_target(dataframe, target, threshold):
    cor = dataframe.corr()
    # Correlation with output variable
    cor_target = abs(cor[target])
    # Selecting non correlated features
    relevant_features = cor_target[cor_target < threshold]
    return relevant_features.index.tolist()[:-1]


# Removing features having low correlation (less than 5%) with the target ('Pass/Fail')
corr_cols = corr_with_target(df, 'Pass/Fail', 0.05)
df = df.drop(corr_cols, axis=1)
print(
    f'After removing {len(corr_cols)} features having low correlation with target, there are {df.shape[1]} features left.')

df.shape

df.head()

"""**Univariate Analysis**"""

df.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12, 12), grid=False)
plt.show()

df.plot(kind='box', subplots=True, layout=(7, 4), sharex=False, sharey=False, legend=False, fontsize=1,
        figsize=(12, 12))
plt.show()

df.plot(kind='density', subplots=True, layout=(7, 4), sharex=False, sharey=False, legend=False, fontsize=1,
        figsize=(12, 12))
plt.show()

"""Insights:
1. Number of features are reduced to 22 making it easier for analysis.
2. The histogram, boxplots and density plots indicate that most of the features are skewed towards right indicating that majority of data values are low with the presence of some larger values.
3. Also some features display two peaks(clusters).

**Multivariate Analysis**
"""

# Plotting pair plots for all columns other than target variable ('Pass/Fail')
plt.figure(figsize=(12, 12), dpi=200)
sns.pairplot(df, kind='scatter', diag_kind='kde', corner=True)

"""Insight:
*  The pairplots show that there is no strong correlation between features.

**Missing value Imputation**
"""

# checking for missing values
df.isnull().sum()

# Since the columns are numeric, replacing missing values with the median for each column
df = df.apply(lambda col: col.fillna(col.median()))

df.isnull().sum()

"""**Feature Encoding**"""

# There is no encoding required, as all the feature columns are float.
# Changing the value of target column, so that each failure is encoded as 0 and 1 corresponds to a pass, for easy understanding
df['Pass/Fail'] = df['Pass/Fail'].replace(to_replace=1, value=0)
df['Pass/Fail'] = df['Pass/Fail'].replace(to_replace=-1, value=1)

# Splitting the data into independent and dependent variables
x = df.drop(['Pass/Fail'], axis=1)
y = df['Pass/Fail']

# Splitting the data into train (used for training the model) and test (used for validating the model)
# We have splitted as 80% training data and 20% test data
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=101, test_size=0.2, stratify=y)
# Stratify is used to divide the dataset such that both train and test data have representation from both the classes (0 and 1) in the same proportion as it is available in the original dataset

x_train.head()

"""**Feature Scaling**"""

# Feature Scaling required, as there is huge difference in the magnitude of values from one column to other
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

x_train = pd.DataFrame(x_train_scaled, columns=x.columns[:])
x_train.head()

x_test = pd.DataFrame(x_test_scaled, columns=x.columns[:])
x_test.head()

"""**Imbalance Treatment**"""

# Imbalance treatement required, since target class is highly imbalanced
# Using SMOTE technique to balance the target classes
smote = SMOTE()
x_smote, y_smote = smote.fit_resample(x_train, y_train)
print("Before Smote data counts : \n", y_train.value_counts())
print("After somte data counts : \n", y_smote.value_counts())

"""# Model Building"""

# Using Stacking Classifier to get combined result from all the classifiers
estimators = []
estimators.append(('LogisticRegression', LogisticRegression(max_iter=1000, random_state=13)))
estimators.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state=13)))
estimators.append(
    ('RandomForest', RandomForestClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=3, random_state=13)))
estimators.append(('AdaBoostClassifier', AdaBoostClassifier(random_state=13)))
estimators.append(('KNN', KNeighborsClassifier()))
estimators.append(('BaggingClassifier', BaggingClassifier(random_state=13)))
estimators.append(('GradientBoostingClassifier', GradientBoostingClassifier(random_state=13)))
estimators.append(('XGB', XGBClassifier(random_state=13)))
estimators.append(('SVC', SVC(random_state=13)))

sc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=10)

sc

sc.fit(x_train, y_train)

# Saving the model to pickle file for deployment
joblib.dump(sc, 'Stacking_model.pkl')

"""# Model Evaluation"""

y_pred_train_sc = sc.predict(x_train)
y_pred_test_sc = sc.predict(x_test)

# Accuracy score
print("Training Accuracy : ", accuracy_score(y_train, y_pred_train_sc))
print()
print("Test Accuracy : ", accuracy_score(y_test, y_pred_test_sc))

# Classification Report
print("Training Classification Report : \n ", classification_report(y_train, y_pred_train_sc))
print()
print("Test Classification Report : \n ", classification_report(y_test, y_pred_test_sc))

# Confusion Matrix
cm_train = confusion_matrix(y_train, y_pred_train_sc)
cm_test = confusion_matrix(y_test, y_pred_test_sc)

train_precision = cm_train[1, 1] / (cm_train[1, 1] + cm_train[0, 1])
test_precision = cm_test[1, 1] / (cm_test[1, 1] + cm_test[0, 1])

train_recall = cm_train[1, 1] / (cm_train[1, 1] + cm_train[1, 0])
test_recall = cm_test[1, 1] / (cm_test[1, 1] + cm_test[1, 0])

fig, ax = plt.subplots(1, 2, figsize=(15, 4))

plot1 = ConfusionMatrixDisplay(cm_train)
plot2 = ConfusionMatrixDisplay(cm_test)

plot1.plot(ax=ax[0])
plot2.plot(ax=ax[1])

ax[0].set_title(f'Train Precision: {train_precision:.2f}, Recall: {train_recall:.2f}')
ax[1].set_title(f'Test: {test_precision:.2f}, Recall: {test_recall:.2f}')

# Precision Recall Curve
# The precision-recall curve shows the tradeoff between precision and recalls for different thresholds.
display = PrecisionRecallDisplay.from_estimator(
    sc, x_test, y_test, name="Average precision")
display.ax_.set_title("Stacking Classifier")

# AUC-ROC curve
y_proba = sc.predict_proba(x_test)
fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
roc_display.figure_.set_size_inches(5, 5)
plt.plot([0, 1], [0, 1], color='g')

"""# Deep Learning approach"""

from sklearn.utils.class_weight import compute_class_weight
import tensorflow
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

# Calculate class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
# Amplify 'FAIL' class weight to 5 times, as the data for failed class is very less
class_weights_dict = {0: class_weights[0], 1: class_weights[1] * 5}

# Display the calculated class weights
print(f"Class Weights: {class_weights_dict}")

# Define the Keras model
model = Sequential()

# Assuming x_train has shape (n_samples, n_features)
input_shape = x_train.shape[1]

# Input Layer
# Increased to 128 neurons for a broader learning capacity
model.add(Dense(128, input_shape=(input_shape,), activation='relu'))
# Added Batch Normalization to stabilize learning and improve convergence
model.add(BatchNormalization())

# First Hidden Layer
model.add(Dense(64, activation='relu'))
# Batch Normalization
model.add(BatchNormalization())

# Reduced dropout to 0.3 for balance between learning and regularization
model.add(Dropout(0.3))

# Second Hidden Layer
model.add(Dense(32, activation='relu'))
# Additional Batch Normalization for more stability in deeper layers
model.add(BatchNormalization())

# Output Layer
# Sigmoid for binary classification output
model.add(Dense(1, activation='sigmoid'))

# Compile the model
# Kept lower learning rate to improve model training stability
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Implement Early Stopping
# Added patience to give the model more room to learn
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=5, verbose=1, restore_best_weights=True)

# Train the model with binary labels
# Increased epochs to allow more training, but rely on early stopping to prevent overfitting
history = model.fit(x_train, y_train, validation_split=0.2, validation_data=(x_test, y_test), epochs=100,
                    callbacks=[early_stopping])

# Plotting the accuracy of the model for train and test data
plt.plot(history.history['accuracy'], color='red', label='train')
plt.plot(history.history['val_accuracy'], color='black', label='test')
plt.legend()
plt.show()

# Plotting the loss from the model for train and test data
plt.plot(history.history['loss'], color='red', label='train')
plt.plot(history.history['val_loss'], color='black', label='test')
plt.legend()
plt.show()

# Evaluate the model on the test set with binary labels
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Get predictions and convert them to binary classes
y_pred = model.predict(x_test)
y_pred_classes = (y_pred > 0.5).astype("int32")

# Generate and print classification report
cr = classification_report(y_test, y_pred_classes, target_names=['PASS', 'FAIL'])
print("Classification Report")
print("---------------------")
print(cr)
